import random

BLANK = ''
AI_PLAYER = 'X'
HUMAN_PLAYER = 'O'
TRAINING_EPOCHS = 40000
TRAINING_EPSILON = 0.4
REWARD_WIN = 10
REWARD_LOSE = -10
REWARD_TIE = -3

class player:
    
    @staticmethod
    def show_board(board):
        print('|'.join(board[0:3]))
        print('|'.join(board[3:6]))
        print('|'.join(board[6:9]))
class HumanPlayer(player):
    def reward(self,value,board):
        pass
    
    def make_move(self,board):
        
        while True:
            try:
                self.show_board(board)
                move = input('Your next move(cell index 1-9):')
                move = int(move)
                if not (move-1 in range(9)):
                    raise ValueError
            except ValueError:
                print('invalid move; try again:\n')
            else:
                    return move-1
class AIPLayer(player):
    def __init__(self,epsilon=0.4,alpha =0.3,gamma=0.9,default_q =1):
        self.EPSILON = epsilon
        self.GAMMA = gamma
        self.ALPHA = alpha
        self.DEFAULT_Q = default_q
        self.q = {}
        self.move = None
        self.board = (' ',)*9
    def available_moves(self,board):
        return[i for i in range(9) if board[i] ==' ']
    def get_q(self,state,action):
        if self.q.get((state,action)) is None:
            self.q[(state,action)] = self.DEFAULT_Q
        return self.q[(state,action)]
    def make_move(self,board):
        
        self.board = tuple(board)
        actions = self.available_moves(board)
        
        if random.random() < self.EPSILON:
            
            self.move = random.choice(actions)
            return self.move
        
        q_values = [self.get_q(self.board,a) for a in actions]
        max_q_value = max(q_values)
        
        if q_values.count(max_q_value)>1:
            
            best_actions =[i for i in range(len(actions)) if q_values[i]==max_q_value]
            best_move = actions[random.choice(best_actions)]
        else:
            best_move = actions[q_values.index(max_q_value)]
            
        self.move = best_move
        return self.move
    
    def reward(self,reward,board):
        if self.move:
            prev_q = self.get_q(self.board,self.move)
            max_q_new = max([self.get_q(tuple(board),a) for a in self.available_moves(self.board)])
            self.q[(self.board,self.move)]= prev_q +self.ALPHA*(reward+self.GAMMA*max_q_new - prev_q)

class TicTacToe:
    
    def __init__(self,player1,player2):
        self.player1 = player1
        self.player2 = player2
        self.first_player_turn = random.choice([True,False])
        self.board = [' '] *9
     
    def play(self):
        
        
        while True:
            
            if self.first_player_turn:
                player = self.player1
                other_player =self.player2
                player_tickers =(AI_PLAYER , HUMAN_PLAYER)
            
            else:
                player = self.player2
                other_player =self.player1
                player_tickers=(HUMAN_PLAYER,AI_PLAYER)
                
            game_over,winner = self.is_game_over(player_tickers)
            
            if game_over:
                if winner == player_tickers[0]:
                    player.show_board(self.board[:])
                    print('\n %s won!'% player.__class__.__name__)
                    player.reward(REWARD_WIN,self.board[:])
                    other_player.reward(REWARD_LOSE,self.board[:])
                    
                if winner == player_tickers[1]:
                    player.show_board(self.board[:])
                    print('\n %s won!'% other_player.__class__.__name__)
                    other_player.reward(REWARD_WIN,self.board[:])
                    player.reward(REWARD_LOSE,self.board[:])
                else:
                    player.show_board(self.board[:])
                    print('Tie!')
                    player.reward(REWARD_TIE,self.board[:])
                    other_player.reward(REWARD_TIE,self.board[:])
                break
            self.first_player_turn = not self.first_player_turn
            
            move = player.make_move(self.board)
            self.board[move] = player_tickers[0]
            
    def is_game_over(self,player_tickers):
                
        for player_ticker in player_tickers:
                    
            for i in range(3):
                if self.board[3 * i + 0] == player_ticker and\
                        self.board[3 * i + 1] == player_ticker and\
                        self.board[3 * i + 2] == player_ticker:
                    return True , player_ticker
                        
            for j in range(3):
                if self.board[ j + 0] == player_ticker and\
                        self.board[ j + 3] == player_ticker and\
                        self.board[ j + 6] == player_ticker:
                    return True , player_ticker
                     
            if self.board[0] == player_ticker and self.board[4] == player_ticker and\
                    self.board[8] == player_ticker:
                return True , player_ticker
            if self.board[2] == player_ticker and self.board[4] == player_ticker and\
                    self.board[6] == player_ticker:
                return True , player_ticker
        if self.board.count(' ') == 0:
            return True , None
        else:
            return False , None
                    
if __name__ == '__main__':
    
    ai_player_1 = AIPLayer()
    ai_player_2 = AIPLayer()
    
    print('The AI  player(s)...')
    ai_player_1.EPSILON = TRAINING_EPSILON
    ai_player_2.EPSILON = TRAINING_EPSILON
    
    
    for _ in range (TRAINING_EPOCHS):
        game = TicTacToe(ai_player_1,ai_player_2)
        game.play()
     
    print('\nTraining is Done')
    
    ai_player_1.EPSILON = 0
    human_player = HumanPlayer()
    game = TicTacToe(ai_player_1,human_player)
    game.play()
    
    



